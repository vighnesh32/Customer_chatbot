{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numexpr in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.8.7)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from numexpr) (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\vigne\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.0.344)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\vigne\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.1,>=0.0.8 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.0.8)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.0.68)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.26.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<4.0->langchain) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: openai in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.7)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (2.5.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (2.14.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\vigne\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Using cached openai_whisper-20231117-py3-none-any.whl\n",
      "Requirement already satisfied: numba in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai-whisper) (0.58.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai-whisper) (1.26.2)\n",
      "Requirement already satisfied: torch in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai-whisper) (2.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai-whisper) (4.66.1)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai-whisper) (10.1.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai-whisper) (0.5.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from numba->openai-whisper) (0.41.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken->openai-whisper) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->openai-whisper) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->openai-whisper) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->openai-whisper) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->openai-whisper) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Installing collected packages: openai-whisper\n",
      "Successfully installed openai-whisper-20231117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\vigne\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\vigne\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\transformers\\\\models\\\\deprecated\\\\trajectory_transformer\\\\convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\vigne\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.6.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (0.16.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: click in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision->sentence-transformers) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Installing collected packages: transformers, sentence-transformers\n",
      "Collecting unstructured\n",
      "  Obtaining dependency information for unstructured from https://files.pythonhosted.org/packages/f8/3d/561a78b6160337f3a8607c183a6ae72352b0b5278b3ff309093c1b3cd211/unstructured-0.11.2-py3-none-any.whl.metadata\n",
      "  Downloading unstructured-0.11.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Obtaining dependency information for chardet from https://files.pythonhosted.org/packages/38/6f/f5fbc992a329ee4e0f288c1fe0e2ad9485ed064cac731ed2fe47dcc38cbf/chardet-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Obtaining dependency information for lxml from https://files.pythonhosted.org/packages/31/58/e3b3dd6bb2ab7404f1f4992e2d0e6926ed40cef8ce1b3bbefd95877499e1/lxml-4.9.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading lxml-4.9.3-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unstructured) (3.8.1)\n",
      "Collecting tabulate (from unstructured)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unstructured) (2.31.0)\n",
      "Collecting beautifulsoup4 (from unstructured)\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting emoji (from unstructured)\n",
      "  Obtaining dependency information for emoji from https://files.pythonhosted.org/packages/03/40/91d0c9fe5a0b494c0fdbcacda4d203aea39f8293e69c70129389308ca928/emoji-2.9.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading emoji-2.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unstructured) (0.6.3)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Obtaining dependency information for python-iso639 from https://files.pythonhosted.org/packages/b8/6d/5d1f7e5c1b0c58b700eb67dbb570f9381afc90bc0535686a89e90eac5dfb/python_iso639-2023.6.15-py3-none-any.whl.metadata\n",
      "  Downloading python_iso639-2023.6.15-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unstructured) (1.26.2)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Obtaining dependency information for rapidfuzz from https://files.pythonhosted.org/packages/89/49/e95e84ed1e5c3c69ea8849d06f8f475c41303533c86710b1c01a4f50d353/rapidfuzz-3.5.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading rapidfuzz-3.5.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unstructured) (4.8.0)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Obtaining dependency information for wrapt from https://files.pythonhosted.org/packages/cf/c3/0084351951d9579ae83a3d9e38c140371e4c6b038136909235079f2e6e78/wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->unstructured)\n",
      "  Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/4c/f3/038b302fdfbe3be7da016777069f26ceefe11a681055ea1f7817546508e3/soupsieve-2.5-py3-none-any.whl.metadata\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json->unstructured) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->unstructured) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->unstructured) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->unstructured) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->unstructured) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->unstructured) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->unstructured) (2023.11.17)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk->unstructured) (0.4.6)\n",
      "Using cached unstructured-0.11.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 199.4/199.4 kB 11.8 MB/s eta 0:00:00\n",
      "Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
      "   ---------------------------------------- 0.0/397.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 397.5/397.5 kB 25.8 MB/s eta 0:00:00\n",
      "Downloading lxml-4.9.3-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.9/3.8 MB 28.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.7/3.8 MB 22.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.5/3.8 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.3/3.8 MB 19.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 14.1 MB/s eta 0:00:00\n",
      "Using cached python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
      "Downloading rapidfuzz-3.5.2-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 31.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 21.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 18.1 MB/s eta 0:00:00\n",
      "Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993254 sha256=831a977c9beb7c5463aa44cc4d894e69d2d57896a16f3a8ac2ccf16659272d4b\n",
      "  Stored in directory: c:\\users\\vigne\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: filetype, wrapt, tabulate, soupsieve, rapidfuzz, python-magic, python-iso639, lxml, langdetect, emoji, chardet, backoff, beautifulsoup4, unstructured\n",
      "Successfully installed backoff-2.2.1 beautifulsoup4-4.12.2 chardet-5.2.0 emoji-2.9.0 filetype-1.2.0 langdetect-1.0.9 lxml-4.9.3 python-iso639-2023.6.15 python-magic-0.4.27 rapidfuzz-3.5.2 soupsieve-2.5 tabulate-0.9.0 unstructured-0.11.2 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\vigne\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Obtaining dependency information for chromadb from https://files.pythonhosted.org/packages/0d/e9/117169df4027a5475ec6406c49f4c5f0ed2b7a7e46d10f02ab5fc049e9ca/chromadb-0.4.18-py3-none-any.whl.metadata\n",
      "  Downloading chromadb-0.4.18-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (2.5.2)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
      "  Obtaining dependency information for chroma-hnswlib==0.7.3 from https://files.pythonhosted.org/packages/d2/32/a91850c7aa8a34f61838913155103808fe90da6f1ea4302731b59e9ba6f2/chroma_hnswlib-0.7.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading chroma_hnswlib-0.7.3-cp311-cp311-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Obtaining dependency information for fastapi>=0.95.2 from https://files.pythonhosted.org/packages/f3/4f/0ce34195b63240b6693086496c9bab4ef23999112184399a3e88854c7674/fastapi-0.104.1-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Obtaining dependency information for uvicorn[standard]>=0.18.3 from https://files.pythonhosted.org/packages/7e/17/4b7a76fffa7babf397481040d8aef2725b2b81ae19f1a31b5ca0c17d49e6/uvicorn-0.24.0.post1-py3-none-any.whl.metadata\n",
      "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Obtaining dependency information for posthog>=2.4.0 from https://files.pythonhosted.org/packages/3b/82/441cb77a43499661228048dcd0d21e0ae3235b442d0f1b9b606e29c2a5ed/posthog-3.1.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading posthog-3.1.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (4.8.0)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
      "  Obtaining dependency information for pulsar-client>=3.1.0 from https://files.pythonhosted.org/packages/72/7f/267fb790b26dab29c0a3cdd8ca1cd0a73ce92200e9d381153f51cd731757/pulsar_client-3.3.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pulsar_client-3.3.0-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Obtaining dependency information for onnxruntime>=1.14.1 from https://files.pythonhosted.org/packages/49/bd/a00f271510098ee62c097ecec663484ff12de632bea1bcaa02ea3679cd03/onnxruntime-1.16.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading onnxruntime-1.16.3-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Obtaining dependency information for opentelemetry-api>=1.2.0 from https://files.pythonhosted.org/packages/51/3a/945e6c21f405ac4ea526f91ee09cc1568c04e0c95d3392903e6984c8f0e0/opentelemetry_api-1.21.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Obtaining dependency information for opentelemetry-exporter-otlp-proto-grpc>=1.2.0 from https://files.pythonhosted.org/packages/75/59/ec3e39fe164c61306998cdd3cd30a857c4da2f8d3141204a929e57668eee/opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Obtaining dependency information for opentelemetry-instrumentation-fastapi>=0.41b0 from https://files.pythonhosted.org/packages/0a/47/411f4dd5560f004c4d59e977fb6e1f511babfc399f3656b6eafd8f67c0ce/opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Obtaining dependency information for opentelemetry-sdk>=1.2.0 from https://files.pythonhosted.org/packages/c3/08/ca8b1ef7a2fa3f1ea2f12770eca8976098066adb442b1da81fea3b370123/opentelemetry_sdk-1.21.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (0.15.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (4.66.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Obtaining dependency information for overrides>=7.3.1 from https://files.pythonhosted.org/packages/da/28/3fa6ef8297302fc7b3844980b6c5dbc71cdbd4b61e9b2591234214d5ab39/overrides-7.4.0-py3-none-any.whl.metadata\n",
      "  Downloading overrides-7.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Obtaining dependency information for importlib-resources from https://files.pythonhosted.org/packages/93/e8/facde510585869b5ec694e8e0363ffe4eba067cb357a8398a55f6a1f8023/importlib_resources-6.1.1-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Obtaining dependency information for grpcio>=1.58.0 from https://files.pythonhosted.org/packages/6a/b9/f94bea4c6f0e322a239f7ba66ba3b0ce766d1c6a2d50055f7c8acf0fba38/grpcio-1.60.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.60.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Obtaining dependency information for bcrypt>=4.0.1 from https://files.pythonhosted.org/packages/c6/d1/7ea6c7e5c864decc4282ac749d1c39a04363443a7ddcbb28f0f3d3370ff8/bcrypt-4.1.1-cp37-abi3-win_amd64.whl.metadata\n",
      "  Downloading bcrypt-4.1.1-cp37-abi3-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Obtaining dependency information for kubernetes>=28.1.0 from https://files.pythonhosted.org/packages/f5/6a/1f69c2d8b1ff03f8d8e10d801f4ac3016ed4c1b00aa9795732c6ec900bba/kubernetes-28.1.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Obtaining dependency information for mmh3>=4.0.1 from https://files.pythonhosted.org/packages/3f/2f/f2a4f4ae2f8eaeb0e3eb0582d8b95f2c1a7cff3250373d41ea8d28b4b14e/mmh3-4.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading mmh3-4.0.1-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (1.26.2)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Obtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for google-auth>=1.0.1 from https://files.pythonhosted.org/packages/f4/d2/9f6f3b9c0fd486617816cff42e856afea079d0bad99f0e60dc186c76b881/google_auth-2.25.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.25.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 from https://files.pythonhosted.org/packages/1e/70/1e88138a9afbed1d37093b85f0bebc3011623c4f47c166431599fe9d6c93/websocket_client-1.7.0-py3-none-any.whl.metadata\n",
      "  Downloading websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for urllib3<2.0,>=1.24.2 from https://files.pythonhosted.org/packages/b0/53/aa91e163dcfd1e5b82d8a890ecf13314e3e149c05270cc644581f77f17fd/urllib3-1.26.18-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.9/48.9 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Obtaining dependency information for flatbuffers from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: packaging in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/fe/6b/7f177e8d6fe4caa14f4065433af9f879d4fab84f0d17dcba7b407f6bd808/protobuf-4.25.1-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: sympy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for deprecated>=1.2.6 from https://files.pythonhosted.org/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl.metadata\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for importlib-metadata<7.0,>=6.0 from https://files.pythonhosted.org/packages/59/9b/ecce94952ab5ea74c31dcf9ccf78ccd484eebebef06019bf8cb579ab4519/importlib_metadata-6.11.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for googleapis-common-protos~=1.52 from https://files.pythonhosted.org/packages/f0/43/c9d8f75ddf08e2a0a27db243c13a700c3cc7ec615b545b697cf6f715ad92/googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-exporter-otlp-proto-common==1.21.0 from https://files.pythonhosted.org/packages/2a/60/ec618caf8fd8a4ac50500565eb49038ec42b7b168df9a316494085a740a6/opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-proto==1.21.0 from https://files.pythonhosted.org/packages/69/c2/d11b5fbf95adf68440ff4c953e2d8d027c9c62ece79b78372af95af590c9/opentelemetry_proto-1.21.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-instrumentation-asgi==0.42b0 from https://files.pythonhosted.org/packages/a9/ef/ac6ef3e2c03750588dab596247204e7f960b509af37f0001ac1e4ac23f2a/opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-instrumentation==0.42b0 from https://files.pythonhosted.org/packages/84/33/8e6b97dcb807c1ba5fd84910c091ae4b1b52d74ea24b0574e19f58cce99c/opentelemetry_instrumentation-0.42b0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-semantic-conventions==0.42b0 from https://files.pythonhosted.org/packages/c3/0c/4c99cbe85b65fbba5a638cb7d913cb3acead3d83b4c47763be28d418bb95/opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-util-http==0.42b0 from https://files.pythonhosted.org/packages/3f/33/7528bd28710e3fd669ee7c5d98bc3392eb4588d451e28352696bb23c1520/opentelemetry_util_http-0.42b0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.5.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for asgiref~=3.0 from https://files.pythonhosted.org/packages/9b/80/b9051a4a07ad231558fcd8ffc89232711b4e618c15cb7a392a17384bbeef/asgiref-3.7.2-py3-none-any.whl.metadata\n",
      "  Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=1.9->chromadb) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.28->chromadb) (3.6)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Obtaining dependency information for httptools>=0.5.0 from https://files.pythonhosted.org/packages/14/e4/20d28dfe7f5b5603b6b04c33bb88662ad749de51f0c539a561f235f42666/httptools-0.6.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading httptools-0.6.1-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Obtaining dependency information for watchfiles>=0.13 from https://files.pythonhosted.org/packages/f3/dc/2a8a447b783f5059c4bf7a6bad8fe59375a5a9ce872774763b25c21c2860/watchfiles-0.21.0-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading watchfiles-0.21.0-cp311-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Obtaining dependency information for websockets>=10.4 from https://files.pythonhosted.org/packages/d1/40/6b169cd1957476374f51f4486a3e85003149e62a14e6b78a958c2222337a/websockets-12.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading websockets-12.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.10.0)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for zipp>=0.5 from https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl.metadata\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for pyasn1<0.6.0,>=0.4.6 from https://files.pythonhosted.org/packages/d1/75/4686d2872bf2fc0b37917cbc8bbf0dd3a5cdb0990799be1b9cbf1e1eb733/pyasn1-0.5.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
      "Downloading chroma_hnswlib-0.7.3-cp311-cp311-win_amd64.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.6/151.6 kB 8.8 MB/s eta 0:00:00\n",
      "Using cached bcrypt-4.1.1-cp37-abi3-win_amd64.whl (158 kB)\n",
      "Using cached fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "Downloading grpcio-1.60.0-cp311-cp311-win_amd64.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.7/3.7 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.5/3.7 MB 19.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.5/3.7 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.5/3.7 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.7/3.7 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.7/3.7 MB 14.8 MB/s eta 0:00:00\n",
      "Using cached kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Downloading mmh3-4.0.1-cp311-cp311-win_amd64.whl (36 kB)\n",
      "Downloading onnxruntime-1.16.3-cp311-cp311-win_amd64.whl (7.4 MB)\n",
      "   ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.9/7.4 MB 18.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.8/7.4 MB 23.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.8/7.4 MB 19.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.7/7.4 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.5/7.4 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.4/7.4 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.4/7.4 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.4/7.4 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.4/7.4 MB 15.7 MB/s eta 0:00:00\n",
      "Using cached opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n",
      "Using cached opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
      "Using cached opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n",
      "Using cached opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
      "Using cached overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
      "Downloading pulsar_client-3.3.0-cp311-cp311-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.9/3.4 MB 27.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.8/3.4 MB 19.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.8/3.4 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.4/3.4 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.4/3.4 MB 16.9 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading google_auth-2.25.2-py2.py3-none-any.whl (184 kB)\n",
      "   ---------------------------------------- 0.0/184.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 184.2/184.2 kB 10.9 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "   ---------------------------------------- 0.0/228.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 228.7/228.7 kB 13.7 MB/s eta 0:00:00\n",
      "Downloading httptools-0.6.1-cp311-cp311-win_amd64.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.4/55.4 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading protobuf-4.25.1-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 12.6 MB/s eta 0:00:00\n",
      "Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.8/143.8 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading watchfiles-0.21.0-cp311-none-win_amd64.whl (280 kB)\n",
      "   ---------------------------------------- 0.0/280.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 280.1/280.1 kB 8.4 MB/s eta 0:00:00\n",
      "Downloading websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.5/58.5 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading websockets-12.0-cp311-cp311-win_amd64.whl (124 kB)\n",
      "   ---------------------------------------- 0.0/125.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 125.0/125.0 kB 7.6 MB/s eta 0:00:00\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "Using cached asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.9/84.9 kB 4.7 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53836 sha256=73f849d59af83c5ac4bf26f16356f3d960d6715e03ca0d4b932bf35684a7b0cd\n",
      "  Stored in directory: c:\\users\\vigne\\appdata\\local\\pip\\cache\\wheels\\a3\\01\\bd\\4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pyreadline3, pypika, monotonic, mmh3, flatbuffers, zipp, websockets, websocket-client, urllib3, python-dotenv, pyasn1, pulsar-client, protobuf, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, oauthlib, importlib-resources, humanfriendly, httptools, grpcio, deprecated, chroma-hnswlib, cachetools, bcrypt, asgiref, watchfiles, uvicorn, typer, starlette, rsa, pyasn1-modules, opentelemetry-proto, importlib-metadata, googleapis-common-protos, coloredlogs, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, google-auth, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "Successfully installed asgiref-3.7.2 bcrypt-4.1.1 cachetools-5.3.2 chroma-hnswlib-0.7.3 chromadb-0.4.18 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.104.1 flatbuffers-23.5.26 google-auth-2.25.2 googleapis-common-protos-1.62.0 grpcio-1.60.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-6.11.0 importlib-resources-6.1.1 kubernetes-28.1.0 mmh3-4.0.1 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.16.3 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.1.0 protobuf-4.25.1 pulsar-client-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 pypika-0.48.9 pyreadline3-3.4.1 python-dotenv-1.0.0 requests-oauthlib-1.3.1 rsa-4.9 starlette-0.27.0 typer-0.9.0 urllib3-1.26.18 uvicorn-0.24.0.post1 watchfiles-0.21.0 websocket-client-1.7.0 websockets-12.0 zipp-3.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\vigne\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install numexpr\n",
    "# !pip install langchain\n",
    "# !pip install openai\n",
    "# !pip install openai-whisper\n",
    "# !pip install sentence-transformers\n",
    "# !pip install unstructured\n",
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'inspect' module, which provides several useful functions to help get information about live objects such as modules, classes, methods, functions, tracebacks, frame objects, and code objects.\n",
    "import inspect\n",
    "\n",
    "# Import the 'random' module, which implements pseudo-random number generators for various distributions. It provides functions for generating random numbers, choosing random elements from sequences, shuffling sequences, etc.\n",
    "import random\n",
    "\n",
    "# Import the 're' module, which provides support for regular expressions. This module allows you to perform operations like searching, matching, and replacing text using regular expressions.\n",
    "import re\n",
    "\n",
    "# Import 'Optional' from the 'typing' module. The 'typing' module supports type hints as specified in PEP 484. 'Optional' is a type hint used to indicate that a variable or return type could be of a specified type or None.\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c219fd4-cd9d-4c88-b8c9-dd9477e5b81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import 'LLMChain' from 'langchain', a class used to create a language model chain. This chain typically involves using a language model to process input and generate output.\n",
    "from langchain import LLMChain\n",
    "\n",
    "# Import multiple chain classes from 'langchain.chains'. These include 'LLMChain' for language model chains, 'LLMMathChain' for chains that handle mathematical computations, 'SequentialChain' for executing a series of chains sequentially, and 'TransformChain' for chains that transform input before passing it to the next chain.\n",
    "from langchain.chains import LLMChain, LLMMathChain, SequentialChain, TransformChain\n",
    "\n",
    "# Import 'ChatOpenAI' from 'langchain.chat_models', which is likely a class for handling chat-based interactions using OpenAI's models.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Import 'OpenAI' from 'langchain.llms'. This is probably a class for interacting with OpenAI's language models, allowing the integration of models like GPT-3 into applications.\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Import 'PydanticOutputParser' from 'langchain.output_parsers'. Pydantic is a data validation and settings management library using Python type annotations. 'PydanticOutputParser' is likely used for parsing and validating the output of language models.\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Import 'PromptTemplate' from 'langchain.prompts'. This is likely a utility for creating and managing prompt templates, which are used to format and structure input for language models.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Import 'BaseModel', 'Field', and 'validator' from 'langchain.pydantic_v1'. These are components of the Pydantic library, used for defining data models with type validation. 'BaseModel' is the base class for model definitions, 'Field' is used for defining model fields, and 'validator' is a decorator for custom validation functions.\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "# Import 'Tool' from 'langchain.tools'. This could be a general class or interface for defining tools within the langchain framework, although without specific documentation, the exact purpose is unclear.\n",
    "from langchain.tools import Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bd8e7-23b0-49a6-8590-3b04c0e4d656",
   "metadata": {},
   "source": [
    "# Langchain 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb60c4a9-624e-461b-88ad-24912c19d4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the 'PromptTemplate' class from the 'langchain' library.\n",
    "# This class is used for creating structured prompts for language models.\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Define a template string for the prompt.\n",
    "# The template includes a placeholder for a 'question' followed by an area for an 'Answer'.\n",
    "# The '{question}' part will be dynamically replaced with the actual content of a question.\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# Create an instance of 'PromptTemplate' using the defined template.\n",
    "# The 'input_variables' parameter is a list of variable names that will be used in the template.\n",
    "# Here, it includes 'question', which is the placeholder in our template.\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Define a new user question, changing the topic to a historical event.\n",
    "# For example, asking about the first human landing on the moon.\n",
    "question = \"When did the first human landing on the Moon occur?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581124ac-afda-4896-8fcd-1fd3dc48407c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: When did the first human landing on the Moon occur?\\n\\nAnswer: '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line is trying to use the 'format' method on the 'prompt' object, which is an instance of 'PromptTemplate'.\n",
    "prompt.format(question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73ce74b-40c8-4667-8d15-0b4626bfeeca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigns the name of the model to be used from OpenAI's offerings to the variable 'model_name'.\n",
    "# Here, \"text-davinci-003\" is specified, which refers to a particular version of OpenAI's language models.\n",
    "model_name = \"text-davinci-003\"\n",
    "\n",
    "# Sets the 'temperature' variable, which controls the randomness in the response generation of the model.\n",
    "# A temperature of 0.0 means the model will generate more deterministic and predictable responses.\n",
    "temperature = 0.0\n",
    "\n",
    "# Assigns an API key for OpenAI's services to the variable 'api_key'.\n",
    "# This key is essential for authentication and usage of OpenAI's API.\n",
    "# The provided key here is a placeholder and should be replaced with an actual API key.\n",
    "api_key = \"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\"  # Replace with your OpenAI API key\n",
    "\n",
    "# Creates an instance of the 'OpenAI' class, passing in the model name, temperature, and API key.\n",
    "# This 'model' instance is configured to interact with OpenAI's API using the specified parameters.\n",
    "# It can be used to make requests to the API for generating text, answering questions, etc.\n",
    "model = OpenAI(model_name=model_name, temperature=temperature, openai_api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f9cab1-20b6-47a2-a2ff-2790f0e066a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This line creates an instance of the 'LLMChain' class from the 'langchain' library.\n",
    "# The 'LLMChain' class is used to create a chain of operations involving a language model (LLM).\n",
    "# In this case, the chain is being configured with two main components:\n",
    "#   1. 'prompt': This is an instance of 'PromptTemplate' (as previously defined in your code).\n",
    "#      It provides the template structure for the input that will be fed into the language model.\n",
    "#   2. 'llm': This is an instance of the 'OpenAI' class (as previously defined in your code).\n",
    "#      It represents the specific language model (in this case, \"text-davinci-003\" from OpenAI)\n",
    "#      and includes settings like temperature and the API key for accessing the model.\n",
    "\n",
    "# The 'LLMChain' thus combines these components to create a pipeline where input data\n",
    "# (structured by 'prompt') is processed by the specified language model ('model').\n",
    "# This chain can then be used to generate language model outputs based on the provided input templates.\n",
    "llm_chain = LLMChain(prompt=prompt, llm=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca10b9ab-8603-4998-978c-4545059b6159",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The first human landing on the Moon occurred on July 20, 1969.\n"
     ]
    }
   ],
   "source": [
    "# This line of code is performing two main operations:\n",
    "\n",
    "# 1. Calling the 'run' method on the 'llm_chain' object:\n",
    "#    - 'llm_chain' is an instance of the 'LLMChain' class from the 'langchain' library.\n",
    "#    - The 'run' method is used to process input through the chain. In this case, the input is 'question'.\n",
    "#    - The method takes the question, formats it according to the prompt template defined in 'llm_chain',\n",
    "#      and then passes it to the language model (in this case, OpenAI's \"text-davinci-003\") for processing.\n",
    "\n",
    "# 2. Printing the result of the 'run' method:\n",
    "#    - The output from the 'run' method is the response generated by the language model.\n",
    "#    - This could be an answer to the question, a continuation of the input text, or any other form of text\n",
    "#      depending on the model's capabilities and configuration.\n",
    "#    - By printing this result, we can see the language model's response to the input question.\n",
    "\n",
    "# Note: The effectiveness and accuracy of the response will depend on various factors such as the quality\n",
    "# of the prompt template, the capabilities of the chosen language model, and the nature of the input question.\n",
    "print(llm_chain.run(question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19155b80-e1f3-4d48-a662-0ec872dcab0a",
   "metadata": {},
   "source": [
    "Asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8099189e-248e-4904-a3fc-bb6852e08831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qs = [\n",
    "    # This question asks about the launch year of the first Apple iPhone, \n",
    "    # which is a query related to technology history.\n",
    "    {\"question\": \"What year did the first Apple iPhone launch?\"},\n",
    "\n",
    "    # This question inquires about the time it takes for light to travel from the Sun to Earth, \n",
    "    # which is a fundamental concept in astronomy.\n",
    "    {\"question\": \"How long does it take for light from the Sun to reach Earth?\"},\n",
    "\n",
    "    # This question is about the author of the novel 'Pride and Prejudice', \n",
    "    # delving into the domain of literature.\n",
    "    {\"question\": \"Who wrote the novel 'Pride and Prejudice'?\"},\n",
    "\n",
    "    # This question asks for the capital city of Japan, \n",
    "    # which is a basic geography-related question.\n",
    "    {\"question\": \"What is the capital city of Japan?\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c789afb6-64ad-4ba2-a4b5-d679f3e7a5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute the 'generate' method on the 'llm_chain' object with 'qs' as its argument.\n",
    "# 'llm_chain': This is an instance of LLMChain, previously set up with a specific prompt and language model.\n",
    "# The 'generate' method is used to process input data through this chain.\n",
    "# 'qs': This is the list of questions, each structured as a dictionary. \n",
    "# Each dictionary contains a key 'question' followed by a question string as its value.\n",
    "# This line will process each question in 'qs' through the language model chain.\n",
    "# The method formats each question according to the prompt template associated with 'llm_chain' \n",
    "# and then utilizes the configured language model (like OpenAI's GPT) to generate a response.\n",
    "# 'res': The results of this operation are stored in the 'res' variable.\n",
    "# These results are expected to be the language model's responses to each of the questions provided in 'qs'.\n",
    "# If 'qs' contains multiple questions, 'res' will typically be a list where each item is the response to the corresponding question in 'qs'.\n",
    "res = llm_chain.generate(qs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94685b7-2348-4a29-86e6-f1e1194dfc13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first Apple iPhone was released in 2007.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing and processing the first generation result from 'res'\n",
    "# 'res': This is the result obtained from the 'llm_chain.generate(qs)' call, where 'qs' is a list of questions.\n",
    "# 'res.generations': It's assumed that 'res' contains a property called 'generations', which is a list of generated responses.\n",
    "# 'res.generations[0]': This accesses the first item in the 'generations' list. This item corresponds to the response for the first question in 'qs'.\n",
    "# 'res.generations[0][0]': Since each item in 'generations' might be a list itself, this accesses the first response in the first list.\n",
    "# '.text': This retrieves the text content of the first response. It's assumed that each response has a 'text' attribute that contains the actual response string.\n",
    "# '.strip()': This is a Python string method that removes any leading and trailing whitespace from the 'text' string. It's useful for cleaning up the response for display or further processing.\n",
    "\n",
    "# The final result is the cleaned text of the first response generated for the first question.\n",
    "res.generations[0][0].text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a0e5f8-40f5-4dae-b0ce-05edaa7946af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It takes about 8 minutes and 20 seconds for light from the Sun to reach Earth.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line of code is accessing the second generated response from the 'res' object and processing it.\n",
    "# 'res': This is the result obtained from the 'llm_chain.generate(qs)' call, where 'qs' is a list of questions.\n",
    "# 'res.generations': This property is assumed to be a list containing the generated outputs for each input question.\n",
    "# 'res.generations[1]': This accesses the second item in the 'generations' list. \n",
    "#                      Since list indices in Python are zero-based, the index 1 corresponds to the second item.\n",
    "#                      This item is the output generated for the second question in your 'qs' list.\n",
    "# 'res.generations[1][0]': This further accesses the first element of the second generated output. \n",
    "#                          If each generation contains multiple responses or parts, this gets the first part of the second response.\n",
    "# '.text': This accesses the 'text' property of the first part of the second generated response, \n",
    "#          which is expected to contain the actual string output from the language model.\n",
    "# '.strip()': This applies the 'strip' method to the text, removing any leading and trailing whitespace from the string.\n",
    "#             It's a standard Python method for string objects, used for cleaning up the output for presentation or further processing.\n",
    "\n",
    "# The final result is the cleaned text of the first part of the second response generated by the language model for the second question.\n",
    "res.generations[1][0].text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "141f8441-0817-4083-bbe1-667bf458b387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jane Austen'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line of code is extracting and cleaning the response for the third question from the 'res' object.\n",
    "# 'res': This is the result object obtained from executing 'llm_chain.generate(qs)', \n",
    "#        where 'qs' is a list of dictionaries, each containing a question.\n",
    "\n",
    "# 'res.generations': It's assumed that 'res' has a property called 'generations', \n",
    "#                    which is a list of lists, each containing the responses generated for each question.\n",
    "\n",
    "# 'res.generations[2]': This accesses the third element in the 'generations' list (indexing starts from 0 in Python).\n",
    "#                       This element corresponds to the generated response(s) for the third question in your 'qs' list.\n",
    "\n",
    "# 'res.generations[2][0]': This accesses the first response (or response part) in the list of responses for the third question.\n",
    "#                          If the generation for each question can have multiple parts or responses, this gets the first part of the third response.\n",
    "\n",
    "# '.text': This accesses the 'text' attribute of the response object, which should contain the actual string response from the language model.\n",
    "\n",
    "# '.strip()': This is a Python string method that removes any leading and trailing whitespace from the response text.\n",
    "#             It is useful for cleaning up the output, especially when presenting it or using it in further processing.\n",
    "\n",
    "# The final result is the cleaned-up text of the first part of the response generated for the third question.\n",
    "res.generations[2][0].text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "093be5eb-ab59-43d7-a833-b653e5a5038b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokyo'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting and cleaning the response text for the fourth question\n",
    "res.generations[3][0].text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae47fe-170c-4582-9f54-b7a28f86cef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What are chains anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c8589-ac5a-46d9-8641-807ddf8d6973",
   "metadata": {},
   "source": [
    "Chains are essential components of this library, as indicated by their fundamental role. A chain consists of links, which are categorized as either basic elements or additional chains. These basic elements include prompts, language model modules (llms), utilities (utils), or can even be chains themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561c9bb-b610-4289-9af0-7d8398dddfc9",
   "metadata": {},
   "source": [
    "Certainly. A chain essentially functions as a processing pipeline, where each segment, known as a \"primitive,\" executes a distinct action on the input. This sequence of primitives can vary widely, from straightforward text modifications using Python functions to intricate processes involving language model (LLM) interactions. The pipeline nature of a chain allows the output of one segment to seamlessly transition into the input for the subsequent segment, culminating in a comprehensively processed result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5181a-d810-4cd4-a671-ad2176f9ebb6",
   "metadata": {},
   "source": [
    "Chains are categorized into three types: Utility chains, Generic chains, and Combine Documents chains. For this edition, our focus will be on the first two types, as the third one is more specialized and will be addressed later.\n",
    "\n",
    "1. Utility Chains: These are chains typically employed to derive a precise answer from a language model (llm) for a narrowly defined purpose. They are designed to be immediately usable without additional setup.\n",
    "2. Generic Chains: These chains serve as foundational components for constructing other chains. However, they are not standalone entities and require further configuration or integration before use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451dbca6-a9bf-42a4-a33f-5baa08fcc81f",
   "metadata": {},
   "source": [
    "### Utility Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6a0b8db-c0f7-43ca-8dc5-41344cb3fdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigne\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain\\chains\\llm_math\\base.py:57: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of the LLMMathChain class.\n",
    "# 'LLMMathChain' is a specialized type of chain, presumably designed to handle mathematical operations or queries.\n",
    "\n",
    "# 'llm=model': This parameter assigns the previously defined 'model' (an instance of a language model, like OpenAI's GPT) to the LLMMathChain.\n",
    "#              The 'model' is used by the LLMMathChain to process inputs and generate outputs.\n",
    "\n",
    "# 'verbose=True': This parameter sets the verbosity mode to True, which likely means that the chain will provide more detailed output or logging.\n",
    "#                 This can be useful for debugging or understanding how the chain processes the input.\n",
    "\n",
    "# 'llm_math': The new instance of LLMMathChain is stored in this variable. \n",
    "#             It can be used to execute mathematical operations using the capabilities of the assigned language model.\n",
    "llm_math = LLMMathChain(llm=model, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae3652a-d519-4990-a581-73306f9c6599",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is the square root of 144?\u001b[32;1m\u001b[1;3m\n",
      "```text\n",
      "144**(1/2)\n",
      "```\n",
      "...numexpr.evaluate(\"144**(1/2)\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m12.0\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 12.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the 'run' method on the 'llm_math' object.\n",
    "# 'llm_math': An instance of LLMMathChain, presumably designed to handle mathematical queries with a language model.\n",
    "# The method 'run' takes a string containing a mathematical question and processes it through the language model.\n",
    "\n",
    "# Changing the question to a different mathematical operation:\n",
    "# For example, let's ask about the square root of 144.\n",
    "llm_math.run(\"What is the square root of 144?\")\n",
    "\n",
    "# The 'run' method will process this new question and the language model will generate a response,\n",
    "# which is expected to be the answer to the mathematical query.\n",
    "# The result of this operation (the answer) will be stored in the variable 'result'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc9841-3737-4e51-9e2e-703cb657c74c",
   "metadata": {},
   "source": [
    "Here, the chain received a question framed in everyday language and forwarded it to the language model (llm). The llm then provided a response in the form of Python code, which the chain executed to produce an answer. This leads to some intriguing questions, such as how the language model determined that our requirement was for it to generate Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c66853-fab7-42ff-8256-1c0f860418fb",
   "metadata": {},
   "source": [
    "#### Enter Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0c362-cc0b-4df8-abc5-697d55110a0e",
   "metadata": {},
   "source": [
    "The input question we provide to the chain isn't the sole piece of information received by the language model (llm). Instead, our input is embedded within a broader context that precisely instructs the llm on how to process our query. This broader context is known as a prompt. Let's take a closer look at what the prompt for this chain entails!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a065d9-f456-49c2-b842-c243eb7ce2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This line is printing the 'template' attribute of the 'prompt' property of the 'llm_math' object.\n",
    "\n",
    "# 'llm_math': An instance of LLMMathChain. It's a specialized chain designed for handling mathematical queries with a language model.\n",
    "\n",
    "# 'llm_math.prompt': This accesses the 'prompt' property of the 'llm_math' object.\n",
    "# The 'prompt' property is expected to hold the prompt configuration that the LLMMathChain uses when sending input to the language model.\n",
    "\n",
    "# 'llm_math.prompt.template': This further accesses the 'template' attribute of the 'prompt' property.\n",
    "# The 'template' attribute contains the actual prompt text or structure that is used by the chain.\n",
    "# This prompt text is what contextualizes the input question for the language model, guiding it on how to interpret and respond to the input.\n",
    "\n",
    "# 'print()': The print function is used to display the content of 'llm_math.prompt.template'.\n",
    "# By printing this out, we can see the specific prompt template that is being used by the LLMMathChain.\n",
    "print(llm_math.prompt.template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee35a8-e405-4b7c-8ba3-e2796de3d51d",
   "metadata": {},
   "source": [
    "Alright, let's delve into what we have here. Essentially, we are instructing the language model (llm) that for intricate mathematical problems, it shouldn't attempt to solve them directly. Instead, it should generate Python code that can compute the solution to the math problem. Likely, if we were to send the query without this contextual framework, the llm might attempt (and possibly fail) to solve it by itself. Hold on, this is something we can actually experiment with and test. Let's give it a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c55fc7ac-082f-4dab-900b-dc5118830cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe factorial of 5 is 120 (5 x 4 x 3 x 2 x 1 = 120).'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up a new prompt template that includes only the question without additional context.\n",
    "# 'PromptTemplate': A class that structures a prompt for the language model.\n",
    "# 'input_variables=[\"question\"]': Specifies that the only input variable is 'question'.\n",
    "# 'template=\"{question}\"': The template is just the question itself, with no extra context or instructions.\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
    "\n",
    "# Create a new instance of LLMChain using the simplified prompt and the pre-defined language model ('model').\n",
    "# 'LLMChain': A class that creates a chain of operations for language model processing.\n",
    "# 'prompt=prompt': Sets the newly created prompt as the template for the chain.\n",
    "# 'llm=model': Uses the previously defined language model for processing.\n",
    "llm_chain = LLMChain(prompt=prompt, llm=model)\n",
    "\n",
    "# Run a mathematical query through the LLMChain without any context.\n",
    "# Changing the question to a different mathematical problem, for example, calculating the factorial of 5.\n",
    "llm_chain.run(\"What is the factorial of 5?\")\n",
    "\n",
    "# The 'run' method sends the input question through the language model chain,\n",
    "# and the result (expected to be the answer or a response to the query) is stored in 'result'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a51e5-d965-49f9-981b-749dc824d097",
   "metadata": {},
   "source": [
    "### Generic Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e1d5d-5f33-437d-92fd-b6a12cd36597",
   "metadata": {},
   "source": [
    "In langchain, there are only three types of Generic Chains, and we're going to demonstrate all of them in a single example. Let's dive in! Imagine we frequently encounter input texts with formatting issues. Notably, since language models (llms) bill based on the number of tokens used, it's inefficient and costly to process inputs with unnecessary characters. Besides, a cleaner input just looks better. Our first step will be to create a custom function to refine the spacing in our texts. Following that, we'll utilize this function to construct a chain. The aim here is to input our text into this chain and receive a neatly formatted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "216e06a8-6739-4d56-969b-69a115c9e79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function 'transform_func' that accepts a dictionary and returns a dictionary.\n",
    "# The input dictionary is expected to contain a key 'text' with a string value.\n",
    "def transform_func(inputs: dict) -> dict:\n",
    "    # Retrieve the text from the input dictionary.\n",
    "    text = inputs[\"text\"]\n",
    "\n",
    "    # Use regular expressions to replace multiple occurrences of new lines \n",
    "    # (including variations like \\r\\n, \\r, and \\n) with a single new line.\n",
    "    # The pattern r\"(\\r\\n|\\r|\\n){2,}\" matches two or more sequential new lines,\n",
    "    # and it's replaced by a single new line r\"\\n\".\n",
    "    text = re.sub(r\"(\\r\\n|\\r|\\n){2,}\", r\"\\n\", text)\n",
    "\n",
    "    # Use another regular expression to replace multiple spaces or tabs \n",
    "    # with a single space. The pattern r\"[ \\t]+\" matches one or more spaces or tabs.\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    # Return the cleaned text in a new dictionary with the key 'output_text'.\n",
    "    return {\"output_text\": text}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa864e6-57ff-4746-8745-bef9434cabf3",
   "metadata": {},
   "source": [
    "Crucially, when setting up the chain, we don't include a language model (llm) as a parameter. This omission naturally limits the capabilities of the chain compared to those we've seen in previous examples with an llm. Nevertheless, as we'll discover, this chain can still achieve impressive outcomes when combined with other chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b01b6119-9779-40a9-8eac-a8bce1f9505e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of the TransformChain class.\n",
    "# TransformChain is used to apply a transformation function to the input data.\n",
    "\n",
    "# 'input_variables=[\"text\"]': This specifies that the input to this chain will be in the form of a dictionary \n",
    "# with a key named 'text'. This is the text that will be transformed.\n",
    "\n",
    "# 'output_variables=[\"output_text\"]': This specifies the expected format of the output from the chain. \n",
    "# The output will be a dictionary with a key named 'output_text', containing the transformed text.\n",
    "\n",
    "# 'transform=transform_func': This sets the transformation function to be used by the chain. \n",
    "# 'transform_func' is the function we defined earlier to clean extra spaces and newlines in the text.\n",
    "\n",
    "# The resulting 'clean_extra_spaces_chain' is a configured TransformChain instance. \n",
    "# It is ready to take text input and return a cleaner version of the text with unnecessary spaces and newlines removed.\n",
    "clean_extra_spaces_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b1ba995-9223-40fd-be09-2aedabc5f6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute the 'run' method on the 'clean_extra_spaces_chain' instance.\n",
    "# 'clean_extra_spaces_chain' is an instance of TransformChain, configured to clean extra spaces and newlines in text.\n",
    "\n",
    "# The argument provided is a string with irregular spacing and newlines:\n",
    "# \"A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.\"\n",
    "# This string has multiple spaces between words and excessive newlines.\n",
    "\n",
    "# The 'run' method processes this string using the transform function defined in 'clean_extra_spaces_chain'.\n",
    "# In this case, the transform function is 'transform_func', which was previously defined to clean up spacing.\n",
    "\n",
    "# The method will return the cleaned text, where:\n",
    "# - Multiple newlines are reduced to a single newline.\n",
    "# - Multiple spaces are reduced to a single space.\n",
    "\n",
    "# The result will be a more neatly formatted version of the input text.\n",
    "cleaned_text = clean_extra_spaces_chain.run(\n",
    "    \"A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f4b7709-aaee-4d15-99e7-80fb6f05bee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a multi-line string 'template' for creating prompts.\n",
    "# This template is designed to ask for a paraphrase of a given text in a specific style.\n",
    "# The template includes placeholders '{output_text}' and '{style}' which will be replaced with actual values.\n",
    "# - The text to be paraphrased is placed after \"Paraphrase this text:\".\n",
    "# - The desired style for the paraphrase is specified in \"In the style of a {style}.\".\n",
    "# - The template ends with \"Paraphrase: \", where the paraphrased response is expected.\n",
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of a {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "\n",
    "# Create an instance of 'PromptTemplate' using the defined template.\n",
    "# 'PromptTemplate' is a class used to format prompts for a language model.\n",
    "# 'input_variables=[\"style\", \"output_text\"]': Specifies the variables 'style' and 'output_text' as inputs for the template.\n",
    "# These variables will be replaced by their respective values when generating a prompt.\n",
    "# 'template=template': Sets the previously defined template as the structure for the prompts.\n",
    "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b17f19d6-adc5-4814-8494-5bfee66b062f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating an instance of the LLMChain class named 'style_paraphrase_chain'.\n",
    "# LLMChain is a class used for creating a sequence of operations involving a language model.\n",
    "\n",
    "# 'llm=model': This parameter sets the language model to be used in the chain.\n",
    "#              The variable 'model' is assumed to be a previously defined and configured language model instance.\n",
    "\n",
    "# 'prompt=prompt': This sets the prompt configuration for the chain.\n",
    "#                  'prompt' is a PromptTemplate instance, as defined earlier.\n",
    "#                  It structures the input for the language model to perform paraphrasing in a specified style.\n",
    "\n",
    "# 'output_key=\"final_output\"': This parameter specifies the key under which the final output of the chain will be stored.\n",
    "#                              When the chain processes an input, the result will be accessible using this key.\n",
    "\n",
    "# The 'style_paraphrase_chain' is thus configured to take input text and a style,\n",
    "# format them according to the 'prompt' template, process it with the 'model' (language model),\n",
    "# and store the paraphrased result under the key 'final_output'.\n",
    "style_paraphrase_chain = LLMChain(llm=model, prompt=prompt, output_key=\"final_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9663730f-f9f9-4ca0-9918-cb419f6edcae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating an instance of SequentialChain named 'sequential_chain'.\n",
    "# SequentialChain is a class used for executing a series of chains in a specified order.\n",
    "\n",
    "# 'chains=[clean_extra_spaces_chain, style_paraphrase_chain]': \n",
    "# This parameter defines the sequence of chains to be executed.\n",
    "# The first chain in the sequence is 'clean_extra_spaces_chain', which cleans up extra spaces in the input text.\n",
    "# The second chain is 'style_paraphrase_chain', which takes the cleaned text and paraphrases it in a specified style.\n",
    "\n",
    "# 'input_variables=[\"text\", \"style\"]': \n",
    "# This specifies the input variables that the SequentialChain expects.\n",
    "# 'text' is the input text to be cleaned and paraphrased, and 'style' is the style in which to paraphrase the text.\n",
    "\n",
    "# 'output_variables=[\"final_output\"]': \n",
    "# This defines the output variables that the SequentialChain will produce.\n",
    "# The final output of the chain sequence will be available under the key 'final_output'.\n",
    "\n",
    "# The 'sequential_chain' will process inputs as follows:\n",
    "# 1. The input text ('text') is first passed through 'clean_extra_spaces_chain' to clean up spacing.\n",
    "# 2. The cleaned text and the specified style ('style') are then passed to 'style_paraphrase_chain' for paraphrasing.\n",
    "# 3. The final paraphrased text is provided as the output under 'final_output'.\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[clean_extra_spaces_chain, style_paraphrase_chain],\n",
    "    input_variables=[\"text\", \"style\"],\n",
    "    output_variables=[\"final_output\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6538b907-a669-461f-8ba4-765523ddd7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a multi-line string 'input_text'.\n",
    "# This text includes various formatting issues like multiple spaces, inconsistent new lines, and irregular spacing.\n",
    "# The content of the text discusses the concept of using chains to combine multiple components in application development.\n",
    "\n",
    "# Discusses the process of making a recipe, including steps and ingredients, with irregular spacing and newlines.\n",
    "input_text = \"\"\"\n",
    "Making a delicious cake involves several steps. First, gather all your ingredients like flour, sugar, \n",
    "\n",
    "\n",
    "and eggs. Then, mix them together carefully, ensuring there are no lumps. \n",
    "\n",
    "It's crucial to bake the cake at the right temperature,       so preheat your oven in advance. \n",
    "\n",
    "After baking, let the cake cool down before    icing or decorating. This process combines \n",
    "\n",
    "\n",
    "culinary skills with creativity.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4baf9120-8dcd-4022-8eed-753619056c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The process of creating a delectable cake requires a few steps. Gather the necessary components such as flour, sugar, and eggs and mix them together thoroughly, making sure there are no clumps. It is essential to set the oven to the correct temperature before baking. Once the cake is finished, let it cool before adding any icing or decorations. This process is a combination of culinary expertise and imagination.\n"
     ]
    }
   ],
   "source": [
    "# Execute the 'run' method on the 'sequential_chain' instance.\n",
    "# 'sequential_chain': A SequentialChain instance that processes input through multiple chains in sequence.\n",
    "\n",
    "# The input for 'run' is a dictionary with two keys:\n",
    "# 'text': Contains the text to be processed, which is 'input_text' defined earlier.\n",
    "#         This text discusses the process of making a cake and includes formatting irregularities.\n",
    "# 'style': Specifies the style in which the text should be paraphrased.\n",
    "#          Initially set to \"a poet\", indicating that the output should be paraphrased in a poetic style.\n",
    "\n",
    "# The 'run' method processes 'input_text' through the 'sequential_chain'.\n",
    "# First, it cleans the text of extra spaces and newlines using 'clean_extra_spaces_chain'.\n",
    "# Then, it paraphrases the cleaned text in the specified style using 'style_paraphrase_chain'.\n",
    "\n",
    "# The result, which is the paraphrased text, is printed out.\n",
    "print(sequential_chain.run({\"text\": input_text, \"style\": \"a science fiction writer\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef7fad-46e5-4b02-abd0-2de7ffd9952b",
   "metadata": {},
   "source": [
    "## Langchain Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e214440-0f49-4d0a-9b79-db752f0edb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" In this snippet, we're defining the data structure for parsing the response from the language model (LLM). The structure includes two components: a setup and a punchline, both of which are strings. These elements are used to shape the prompt sent to the LLM. Notably, this example includes a validator that ensures the setup contains a question mark.\n",
    "\n",
    "Source: [Langchain Python Documentation - Pydantic Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define a class 'Joke' which inherits from 'BaseModel' from the Pydantic library.\n",
    "# Pydantic BaseModel provides functionality for data validation and serialization.\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    # Define a class attribute 'setup' as a string.\n",
    "    # It represents the setup part of a joke and is expected to be a question.\n",
    "    # The 'Field' function is used to add a description to this attribute.\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "\n",
    "    # Define another class attribute 'punchline' as a string.\n",
    "    # It represents the punchline or answer part of the joke.\n",
    "    # Similarly, a description is added to this attribute using 'Field'.\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # Define a class method that acts as a validator for the 'setup' attribute.\n",
    "    # This is a decorator method provided by Pydantic to validate data.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        # Check if the 'setup' string ends with a question mark.\n",
    "        # If not, raise a ValueError indicating the question is badly formed.\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        # If the validation passes, return the 'setup' string.\n",
    "        return field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "868183c3-0de5-4c76-8e71-271bda514b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the user's request for a joke\n",
    "joke_query = \"Tell me a joke about elephants\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc6c94d8-dec3-4a4d-82a0-83a6179cc268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the user query.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n",
      "Tell me a joke about elephants\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the PydanticOutputParser class from langchain's output parsers module.\n",
    "# This parser is designed to convert the output from a language model into a specific format defined by a Pydantic model.\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Initialize an instance of PydanticOutputParser with the 'Joke' class as the Pydantic model.\n",
    "# The 'Joke' class is expected to be a predefined Pydantic model that structures the output data.\n",
    "# In this case, it will be used to parse joke responses from the LLM.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Create a PromptTemplate instance.\n",
    "# This template structures the prompt that will be sent to the LLM.\n",
    "# The template includes:\n",
    "# - A placeholder '{format_instructions}' for instructions on formatting the LLM's response (provided by the parser).\n",
    "# - A placeholder '{query}' for the user's question or request.\n",
    "# 'input_variables=[\"query\"]' specifies 'query' as a variable that will be provided when the prompt is used.\n",
    "# 'partial_variables' sets static parts of the prompt. Here, it includes format instructions for the LLM's response.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Generate the actual prompt using the provided 'joke_query'.\n",
    "# The 'format_prompt' method replaces '{query}' with the content of 'joke_query'.\n",
    "_input = prompt.format_prompt(query=joke_query)\n",
    "\n",
    "# Print the formatted prompt.\n",
    "# This output shows the complete prompt that will be sent to the LLM, containing the user's joke query\n",
    "# and the specific format instructions for the LLM to structure its response.\n",
    "print(_input.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d69c93e-5751-49ab-a6d7-5390732b33dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sending a request to the language model and parsing its output.\n",
    "\n",
    "# 'output = model(_input.to_string())': \n",
    "# This line sends a request to the language model 'model'.\n",
    "# 'model': This is presumably a pre-initialized instance of a language model, ready to process inputs.\n",
    "# '_input.to_string()': Converts the '_input' object, which is a formatted prompt, into a string.\n",
    "#                       This string is the actual input that the language model will process.\n",
    "\n",
    "# The language model processes this input and returns a response, which is stored in 'output'.\n",
    "output = model(_input.to_string())\n",
    "\n",
    "# 'parser.parse(output)': \n",
    "# This line uses the 'parser' object to parse the 'output' from the language model.\n",
    "# 'parser': An instance of PydanticOutputParser, initialized earlier with the 'Joke' Pydantic model.\n",
    "#           It's designed to convert the raw output of the language model into a structured format.\n",
    "# The 'parse' method of 'parser' takes the raw output from 'model' and structures it according to the 'Joke' model.\n",
    "# This structured data can be used for easier handling and processing of the language model's response.\n",
    "parsed_output = parser.parse(output)\n",
    "\n",
    "# The result, 'parsed_output', is the language model's response formatted as per the 'Joke' model's structure.\n",
    "# This can include structured fields like the setup and punchline of a joke.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ac28e-4d50-421c-ab0c-44f43f476a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c4a7dfe-c66c-4b7e-a5ef-d5c84f929155",
   "metadata": {},
   "source": [
    "# LLMs as Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d1313d9-0bcb-4587-bb3d-78b0ce383349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigns the name of the model to be used from OpenAI's offerings to the variable 'model_name'.\n",
    "# Here, \"text-davinci-003\" is specified, which refers to a particular version of OpenAI's language models.\n",
    "model_name = \"text-davinci-003\"\n",
    "\n",
    "# Sets the 'temperature' variable, which controls the randomness in the response generation of the model.\n",
    "# A temperature of 0.0 means the model will generate more deterministic and predictable responses.\n",
    "temperature = 0.0\n",
    "\n",
    "# Assigns an API key for OpenAI's services to the variable 'api_key'.\n",
    "# This key is essential for authentication and usage of OpenAI's API.\n",
    "# The provided key here is a placeholder and should be replaced with an actual API key.\n",
    "api_key = \"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\"  # Replace with your OpenAI API key\n",
    "\n",
    "# Creates an instance of the 'OpenAI' class, passing in the model name, temperature, and API key.\n",
    "# This 'model' instance is configured to interact with OpenAI's API using the specified parameters.\n",
    "# It can be used to make requests to the API for generating text, answering questions, etc.\n",
    "model = OpenAI(model_name=model_name, temperature=temperature, openai_api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6ce8561-27f4-4150-ad7b-a3489aa23b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the BaseModel class from the Pydantic library.\n",
    "# BaseModel is used in Pydantic for creating data models with built-in validation.\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a new class 'sampleOutputTemplate' that inherits from Pydantic's BaseModel.\n",
    "# This class serves as a data model for creating structured objects with validation.\n",
    "class sampleOutputTemplate(BaseModel):\n",
    "    # Declare a class attribute 'output' of type string.\n",
    "    # The 'Field' decorator is used to add metadata to this attribute, particularly a description.\n",
    "    # The description provided here is \"contact information\", which suggests the intended content for 'output'.\n",
    "    # This attribute will hold a string, and the description aids in understanding its purpose in the model.\n",
    "    output: str = Field(description=\"contact information\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4598a-ac46-45ab-afe5-86cf5857f67a",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d4e2a85-dbc0-4168-91c6-6e6efa90d525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define utility functions for managing the flow of a conversational exchange.\n",
    "\n",
    "def system_output(output):\n",
    "    \"\"\"Function for printing out to the user\"\"\"\n",
    "    # Prints a separator and label to indicate the start of the bot's response.\n",
    "    print(\"======= Bot =======\")\n",
    "    # Prints the actual output from the bot or system.\n",
    "    # The 'output' parameter is expected to be a string containing the bot's response or message.\n",
    "    print(output)\n",
    "\n",
    "def user_input():\n",
    "    \"\"\"Function for getting user input\"\"\"\n",
    "    # Prints a separator and label to indicate that it's the user's turn to input.\n",
    "    print(\"======= Human Input =======\")\n",
    "    # Waits for and returns the user's input.\n",
    "    # The 'input()' function is a built-in Python function that reads a line from input.\n",
    "    return input()\n",
    "\n",
    "def parsing_info(output):\n",
    "    \"\"\"Function for printing out key info\"\"\"\n",
    "    # Prints additional information, often for debugging or providing insights.\n",
    "    # The 'output' parameter can contain any relevant information to be displayed.\n",
    "    print(f\"*Info* {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae788b-b042-4e79-9597-e3cbb72a2bec",
   "metadata": {},
   "source": [
    "## Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b432cda-68af-4f4f-b0db-e2229f5133ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define a class named 'Edge'.\n",
    "# The purpose of an 'Edge' is to evaluate whether an input meets a certain condition and then extract data from the input if it does.\n",
    "\n",
    "class Edge:\n",
    "    \"\"\"Edge\n",
    "    At its highest level, an edge checks if an input is good, then parses\n",
    "    data out of that input if it is good.\n",
    "    \"\"\"\n",
    "\n",
    "    # Constructor of the Edge class.\n",
    "    def __init__(\n",
    "        self, condition, parse_prompt, parse_class, llm, max_retrys=3, out_node=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Edge object with specific parameters.\n",
    "        \n",
    "        Args:\n",
    "            condition (str): A True/False question about the input.\n",
    "            parse_query (str): What the parser should be extracting.\n",
    "            parse_class (Pydantic BaseModel): The structure of the parsed data.\n",
    "            llm (LangChain LLM): The large language model being used.\n",
    "            max_retrys (int): Maximum number of retries for the edge.\n",
    "            out_node: The next node in the process flow.\n",
    "        \"\"\"\n",
    "        self.condition = condition\n",
    "        self.parse_prompt = parse_prompt\n",
    "        self.parse_class = parse_class\n",
    "        self.llm = llm\n",
    "        self.max_retrys = max_retrys\n",
    "        self.out_node = out_node\n",
    "        self.num_fails = 0  # Track the number of failures.\n",
    "\n",
    "    # Method to check if the input satisfies the specified condition.\n",
    "    def check(self, input):\n",
    "        \"\"\"Ask the llm if the input satisfies the condition.\"\"\"\n",
    "        # Construct a validation query for the LLM.\n",
    "        validation_query = f\"Following the output schema, does the input satisfy the condition?\\ninput:{input}\\ncondition:{self.condition}\"\n",
    "\n",
    "        # Define a Pydantic model for validation.\n",
    "        class Validation(BaseModel):\n",
    "            is_valid: bool = Field(description=\"If the condition is satisfied\")\n",
    "\n",
    "        # Parse the LLM's response to check if the condition is met.\n",
    "        parser = PydanticOutputParser(pydantic_object=Validation)\n",
    "        formatted_input = f\"Answer the user query.\\n{parser.get_format_instructions()}\\n{validation_query}\\n\"\n",
    "        return parser.parse(self.llm(formatted_input)).is_valid\n",
    "\n",
    "    # Method to parse the input based on the parse_class and parse_prompt.\n",
    "    def parse(self, input):\n",
    "        \"\"\"Ask the llm to parse the parse_class, based on the parse_prompt, from the input.\"\"\"\n",
    "        # Construct the query for parsing.\n",
    "        parse_query = f'{self.parse_prompt}:\\n\\n\"{input}\"'\n",
    "        parser = PydanticOutputParser(pydantic_object=self.parse_class)\n",
    "        formatted_input = f\"Answer the user query.\\n{parser.get_format_instructions()}\\n{parse_query}\\n\"\n",
    "        return parser.parse(self.llm(formatted_input))\n",
    "\n",
    "    # Main method to execute the edge's logic.\n",
    "    def execute(self, input):\n",
    "        \"\"\"Executes the entire edge process.\"\"\"\n",
    "        # Check the input against the edge's condition.\n",
    "        if not self.check(input):\n",
    "            self.num_fails += 1\n",
    "            # Decide whether to continue based on the number of failures.\n",
    "            if self.num_fails >= self.max_retrys:\n",
    "                return {\"continue\": True, \"result\": None, \"num_fails\": self.num_fails}\n",
    "            return {\"continue\": False, \"result\": None, \"num_fails\": self.num_fails}\n",
    "\n",
    "        try:\n",
    "            # Attempt to parse the input if the condition is met.\n",
    "            self.num_fails = 0\n",
    "            return {\n",
    "                \"continue\": True,\n",
    "                \"result\": self.parse(input),\n",
    "                \"num_fails\": self.num_fails,\n",
    "            }\n",
    "        except:\n",
    "            # Handle any parsing errors.\n",
    "            self.num_fails += 1\n",
    "            if self.num_fails >= self.max_retrys:\n",
    "                return {\"continue\": True, \"result\": None, \"num_fails\": self.num_fails}\n",
    "            return {\"continue\": False, \"result\": None, \"num_fails\": self.num_fails}\n",
    "\n",
    "# The 'Edge' class represents a conditional step in a processing pipeline.\n",
    "# It integrates querying a language model to check conditions and parse data,\n",
    "# with mechanisms for handling retries and failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f0edce8-df67-4a67-a806-a42bc589c0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up query conditions and parsing instructions for a text processing task.\n",
    "\n",
    "# 'condition' variable:\n",
    "# This is a string defining a specific condition to be checked against an input.\n",
    "# In this case, the condition is \"Does the input contain fruits?\".\n",
    "# This condition could be used, for instance, in a language model query to determine \n",
    "# if the provided input text includes any mention of fruits.\n",
    "condition = \"Does the input contain fruits?\"\n",
    "\n",
    "# 'parse_prompt' variable:\n",
    "# This string defines the instructions for parsing the input text.\n",
    "# The given instructions are \"extract only the fruits from the following text. \n",
    "# Do not extract any food items besides pure fruits.\".\n",
    "# This prompt could be used to guide a language model in extracting only fruit-related items \n",
    "# from a given text, explicitly excluding other food items.\n",
    "parse_prompt = \"extract only the fruits from the following text. Do not extract any food items besides pure fruits.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76e75004-3356-4826-b0c0-1c1766cbd67f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of the 'Edge' class for text processing and condition checking.\n",
    "\n",
    "# 'testEdge': The variable name for the new Edge instance.\n",
    "# 'Edge': The class previously defined, which handles checking if an input meets a condition and parsing data from the input.\n",
    "\n",
    "# Parameters passed to the Edge constructor:\n",
    "# 'condition=condition': Sets the condition that the edge will check against the input.\n",
    "#                         The 'condition' variable was defined earlier as \"Does the input contain fruits?\".\n",
    "#                         It's a criterion to determine if the input text meets certain requirements (contains fruits in this case).\n",
    "\n",
    "# 'parse_prompt=parse_prompt': Specifies the instructions for parsing the input.\n",
    "#                              The 'parse_prompt' variable contains instructions to extract only fruits from the text.\n",
    "#                              It guides how the input should be processed and what data should be extracted.\n",
    "\n",
    "# 'parse_class=sampleOutputTemplate': Sets the structure for the parsed data.\n",
    "#                                     'sampleOutputTemplate' is presumably a Pydantic model defined earlier, \n",
    "#                                     which structures the output data (in this case, regarding fruits extraction).\n",
    "\n",
    "# 'llm=model': Specifies the language model to be used for processing the input.\n",
    "#              'model' is assumed to be a pre-initialized language model instance (like OpenAI's GPT model).\n",
    "\n",
    "# The resulting 'testEdge' object is configured to process inputs based on the specified condition and parsing instructions.\n",
    "# It uses the provided language model ('model') to evaluate and parse the input according to these configurations.\n",
    "testEdge = Edge(\n",
    "    condition=condition,\n",
    "    parse_prompt=parse_prompt,\n",
    "    parse_class=sampleOutputTemplate,\n",
    "    llm=model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dbb0dcb-a72e-4710-bbd0-d753a8ec44c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a sample input string from a user.\n",
    "\n",
    "# 'sample_input': The variable name for storing the sample text.\n",
    "# The text provided here is \"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\"\n",
    "# This string lists several desserts, including both fruit-based and other types of sweet dishes.\n",
    "# The text is formatted as a casual statement, resembling a typical user input in a conversational or text processing application.\n",
    "\n",
    "# The content of this variable can be used as input for processing tasks, \n",
    "# such as checking conditions or extracting specific information based on previously defined criteria.\n",
    "# In the context of the earlier code snippets, this input could be used with the 'testEdge' object to identify \n",
    "# and extract mentions of fruits from the text, based on the defined conditions and parsing instructions.\n",
    "sample_input = \"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6729e7bf-acf2-44de-8da6-41ab9f8b5237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== testing the condition functionality =====\n",
      "whether or not the input \n",
      "\"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\"\n",
      "satisfies the condition\n",
      "\"Does the input contain fruits?\"\n",
      "result: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing the condition checking functionality of the 'testEdge' object.\n",
    "\n",
    "# Print a header to indicate the start of the test for the condition functionality.\n",
    "print(\"===== testing the condition functionality =====\")\n",
    "\n",
    "# Format and print the input text and the condition being tested.\n",
    "# The formatted string includes:\n",
    "# - The sample input text ('sample_input') which is a user-provided string of favorite desserts.\n",
    "# - The condition ('condition') that is being checked against the input.\n",
    "# This helps in understanding what exactly is being tested.\n",
    "print(\n",
    "    f'whether or not the input \\n\"{sample_input}\"\\nsatisfies the condition\\n\"{condition}\"'\n",
    ")\n",
    "\n",
    "# Call the 'check' method of 'testEdge' with 'sample_input' and print the result.\n",
    "# 'testEdge.check(sample_input)' evaluates whether the input satisfies the specified condition.\n",
    "# The result is a boolean value indicating if the condition is met.\n",
    "# It is formatted into a string for display.\n",
    "# 'result: {}' prints the outcome of the condition check, showing whether the input meets the condition.\n",
    "print(\"result: {}\".format(testEdge.check(sample_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27ce6bb8-35e0-4f9d-89b7-ffbacd0ec35e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== testing the condition functionality =====\n",
      "whether or not the input \n",
      "\"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\"\n",
      "satisfies the condition\n",
      "\"Does the input contain fruits?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: True\n"
     ]
    }
   ],
   "source": [
    "# Testing the condition checking functionality of the 'testEdge' object.\n",
    "\n",
    "# Print a header to indicate the start of the test for the condition functionality.\n",
    "print(\"===== testing the condition functionality =====\")\n",
    "\n",
    "# Format and print the input text and the condition being tested.\n",
    "# The formatted string includes:\n",
    "# - The sample input text ('sample_input') which is a user-provided string of favorite desserts.\n",
    "# - The condition ('condition') that is being checked against the input.\n",
    "# This helps in understanding what exactly is being tested.\n",
    "print(\n",
    "    f'whether or not the input \\n\"{sample_input}\"\\nsatisfies the condition\\n\"{condition}\"'\n",
    ")\n",
    "\n",
    "# Call the 'check' method of 'testEdge' with 'sample_input' and print the result.\n",
    "# 'testEdge.check(sample_input)' evaluates whether the input satisfies the specified condition.\n",
    "# The result is a boolean value indicating if the condition is met.\n",
    "# It is formatted into a string for display.\n",
    "# 'result: {}' prints the outcome of the condition check, showing whether the input meets the condition.\n",
    "print(\"result: {}\".format(testEdge.check(sample_input)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673050f-5b67-4f0f-90b7-f9da00e3629b",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ce4fa11-e8f9-477c-9c33-01eb0c5b6443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a class 'Node' to handle conversational states and manage edges in a dialogue system.\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Node\n",
    "    At its highest level, a node asks a user for some input and tries\n",
    "    that input on all edges. It also manages and executes all\n",
    "    the edges it contains.\n",
    "    \"\"\"\n",
    "\n",
    "    # Constructor for the Node class.\n",
    "    def __init__(self, prompt, retry_prompt):\n",
    "        \"\"\"\n",
    "        Initialize the Node object with specific parameters.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The text to display to the user initially.\n",
    "            retry_prompt (str): The text to display if all edges fail and a retry is needed.\n",
    "        \"\"\"\n",
    "\n",
    "        self.prompt = prompt  # The initial prompt to display to the user.\n",
    "        self.retry_prompt = retry_prompt  # The prompt for retrying if edges fail.\n",
    "        self.edges = []  # A list to store edges associated with this node.\n",
    "\n",
    "    # Method to run all edges until one allows for continuation.\n",
    "    def run_to_continue(self, _input):\n",
    "        \"\"\"Run all edges until one continues.\n",
    "        Returns the result of the continuing edge, or None.\n",
    "        \"\"\"\n",
    "        for edge in self.edges:\n",
    "            # Execute each edge with the input.\n",
    "            res = edge.execute(_input)\n",
    "            # If an edge allows for continuation, return its result.\n",
    "            if res[\"continue\"]:\n",
    "                return res\n",
    "        # If no edge allows for continuation, return None.\n",
    "        return None\n",
    "\n",
    "    # Main method to handle the conversational state.\n",
    "    def execute(self):\n",
    "        \"\"\"Handles the current conversational state.\n",
    "        Prompts the user, tries again, runs edges, etc.\n",
    "        Returns the result from an edge.\n",
    "        \"\"\"\n",
    "\n",
    "        # Display the initial prompt to the user.\n",
    "        system_output(self.prompt)\n",
    "\n",
    "        # Enter a loop to handle user input and edge execution.\n",
    "        while True:\n",
    "            # Get user input.\n",
    "            _input = user_input()\n",
    "\n",
    "            # Run through edges with the input.\n",
    "            res = self.run_to_continue(_input)\n",
    "\n",
    "            # If an edge returns a result, display parsing info and return the result.\n",
    "            if res is not None:\n",
    "                parsing_info(f\"parse results: {res}\")\n",
    "                return res\n",
    "\n",
    "            # If no edge returned a result, display the retry prompt.\n",
    "            system_output(self.retry_prompt)\n",
    "\n",
    "# The 'Node' class is a key component in managing conversational flows, \n",
    "# where it prompts users for input, processes the input through various edges, \n",
    "# and handles retry scenarios based on the edges' responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f8c11-1d0a-4fae-b59c-486fda7c27aa",
   "metadata": {},
   "source": [
    "## Connecting Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "012920d9-c60a-4d25-ab22-29a16eed3670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up two distinct edges for processing different types of user input.\n",
    "\n",
    "# Define the condition for the first edge.\n",
    "# 'condition1' checks if the user's input contains a valid email address.\n",
    "condition1 = \"Does the input contain a full and valid email?\"\n",
    "\n",
    "# Define the parsing instructions for the first edge.\n",
    "# 'parse_prompt1' directs the edge to extract an email address from the input text.\n",
    "parse_prompt1 = \"extract the email from the following text.\"\n",
    "\n",
    "# Initialize the first edge 'edge1' using the specified condition and parsing prompt.\n",
    "# This edge will use the 'sampleOutputTemplate' for structuring the parsed output and 'model' as the language model.\n",
    "edge1 = Edge(condition1, parse_prompt1, sampleOutputTemplate, model)\n",
    "\n",
    "# Define the condition for the second edge.\n",
    "# 'condition2' checks if the input contains a valid phone number in specific formats.\n",
    "condition2 = (\n",
    "    \"Does the input contain a full and valid phone number (xxx-xxx-xxxx or xxxxxxxxxx)?\"\n",
    ")\n",
    "\n",
    "# Define the parsing instructions for the second edge.\n",
    "# 'parse_prompt2' instructs the edge to extract a phone number from the input.\n",
    "parse_prompt2 = \"extract the phone number from the following text.\"\n",
    "\n",
    "# Initialize the second edge 'edge2' with the condition for phone numbers, parsing prompt,\n",
    "# the same output template, and language model as the first edge.\n",
    "edge2 = Edge(condition2, parse_prompt2, sampleOutputTemplate, model)\n",
    "\n",
    "# 'edge1' and 'edge2' are now configured to process inputs specific to their conditions.\n",
    "# 'edge1' focuses on extracting email addresses, while 'edge2' is set up to extract phone numbers.\n",
    "# Both edges use the same output template and language model but apply them to different parsing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d4ca7454-b6db-47d7-9174-b3e77c4432ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up an instance of the Node class for handling user input and managing edges.\n",
    "\n",
    "# Initialize 'test_node' as an instance of the Node class.\n",
    "# 'Node': A class previously defined to handle conversational states and manage the execution of edges.\n",
    "\n",
    "# Parameters passed to the Node constructor:\n",
    "# 'prompt=\"Please input your full email address or phone number\"':\n",
    "#   This is the initial prompt that will be displayed to the user.\n",
    "#   It instructs the user to provide either a full email address or phone number.\n",
    "#   This prompt sets the context for the type of information the node is expecting from the user.\n",
    "\n",
    "# 'retry_prompt=\"I'm sorry, I didn't understand your response.\\nPlease provide a full email address or phone number(in the format xxx-xxx-xxxx)\"':\n",
    "#   This is the prompt used if the user's initial response doesn't satisfy the conditions of the edges within the node.\n",
    "#   It serves as a clarification and reiteration of what is expected, providing an additional format hint for phone numbers.\n",
    "\n",
    "# The 'test_node' object is now ready to interact with users, guiding them to provide specific types of information.\n",
    "# It will use the defined prompts to request and re-request information, depending on the responses received and processed by its edges.\n",
    "test_node = Node(\n",
    "    prompt=\"Please input your full email address or phone number\",\n",
    "    retry_prompt=\"I'm sorry, I didn't understand your response.\\nPlease provide a full email address or phone number(in the format xxx-xxx-xxxx)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21a5c026-10a5-4b68-8973-728a75ceddf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigning the previously defined edges to the 'test_node'.\n",
    "\n",
    "# 'test_node': An instance of the Node class, which was defined earlier.\n",
    "# The Node class is designed to manage and execute a series of edges based on user input.\n",
    "\n",
    "# 'edges': An attribute of the Node instance that holds a list of Edge instances.\n",
    "# Each Edge instance in this list represents a conditional action or parsing operation.\n",
    "\n",
    "# '[edge1, edge2]': A list containing the two Edge instances 'edge1' and 'edge2'.\n",
    "# 'edge1' and 'edge2' were defined earlier with specific conditions and parsing instructions:\n",
    "# - 'edge1' is set up to process inputs related to email addresses.\n",
    "# - 'edge2' is configured to handle inputs containing phone numbers.\n",
    "\n",
    "# By setting 'test_node.edges = [edge1, edge2]', we are specifying that 'test_node' should manage these two edges.\n",
    "# When 'test_node' receives input, it will attempt to process the input through each of these edges in sequence.\n",
    "\n",
    "# This configuration allows 'test_node' to handle different types of input (emails or phone numbers in this case)\n",
    "# and apply the appropriate processing logic as defined in each edge.\n",
    "test_node.edges = [edge1, edge2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1714a934-adfd-4e3c-a0a3-649c396313d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Bot =======\n",
      "Please input your full email address or phone number\n",
      "======= Human Input =======\n",
      "======= Bot =======\n",
      "I'm sorry, I didn't understand your response.\n",
      "Please provide a full email address or phone number(in the format xxx-xxx-xxxx)\n",
      "======= Human Input =======\n"
     ]
    }
   ],
   "source": [
    "# Execute the logic defined in the 'test_node' and store the result.\n",
    "\n",
    "# 'test_node': An instance of the Node class, previously defined and configured with edges and prompts.\n",
    "# The Node class handles conversational states, prompts users for input, and manages edges for processing the input.\n",
    "\n",
    "# 'execute()': A method of the Node class.\n",
    "# This method orchestrates the interaction with the user based on the Node's configuration.\n",
    "# It prompts the user for input, processes that input through the Node's edges, and handles retry scenarios.\n",
    "\n",
    "# The method works as follows:\n",
    "# 1. It displays the initial prompt to the user.\n",
    "# 2. It collects user input and attempts to process it through the Node's edges.\n",
    "# 3. If none of the edges successfully process the input, the Node displays the retry prompt and asks for input again.\n",
    "# 4. This loop continues until an edge successfully processes the input or a certain condition is met (like a maximum number of retries).\n",
    "\n",
    "# 'res': A variable to store the result from 'test_node.execute()'.\n",
    "# The result could be the output from a successful edge processing, or it might indicate that no edge could process the input successfully.\n",
    "\n",
    "# This line of code is crucial in a conversational or interactive system where user input needs to be processed and responded to dynamically.\n",
    "res = test_node.execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddc960-8c07-4889-9524-f2575a79f546",
   "metadata": {},
   "source": [
    "# Customer Support flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fee75c9e-9277-4b89-a10c-ba0b05d17743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data.chat import *\n",
    "from data.graph import *\n",
    "from data.validation import PhoneCallTicket, UserProfile, PhoneCallRequest\n",
    "from graph.chain_based_edge import *\n",
    "from graph.chain_based_node import *\n",
    "from graph.edge import BaseEdge\n",
    "from graph.node import BaseNode\n",
    "from graph.text_based_edge import PydanticTextBasedEdge\n",
    "from tools.rag_responder import HelpCenterAgent\n",
    "from tools.user_info_db import search_user_info_on_db, search_user_subscription_on_db\n",
    "# from tools.audio_transcribe import call_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2b6a521-618c-45e0-bf40-2bb959838d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the language model using ChatOpenAI with specific settings\n",
    "llm_model = ChatOpenAI(\n",
    "    temperature=0,                # Set the temperature parameter\n",
    "    model_name=\"gpt-3.5-turbo\",   # Specify the model name\n",
    "    openai_api_key = \"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\"  # Provide the OpenAI API key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27aa0134-1b04-47fa-8668-409e6263bbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function called 'get_message_history' that takes two parameters.\n",
    "def get_message_history(msg_input: str, role: Role):\n",
    "    # Create an empty message history object.\n",
    "    message_history = MessageHistory([])\n",
    "    \n",
    "    # Add a message to the message history with the provided input and role.\n",
    "    message_history.add_message(msg_input, role)\n",
    "    \n",
    "    # Return the populated message history.\n",
    "    return message_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b645d2d-55c8-42e5-8e12-084eb4706446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a class called 'GreetingNode' that inherits from 'BaseNode' and works with strings.\n",
    "class GreetingNode(BaseNode[str]):\n",
    "    # Define a static prompt for greeting.\n",
    "    STATIC_PROMPT = [\n",
    "        \"Hi, welcome to our online support, in order to proceed we need to identify you first, \"\n",
    "        \"could you please input your full email address or phone number\"\n",
    "    ]\n",
    "    # Define a retry prompt for when no valid response is received.\n",
    "    RETRY_PROMPT = [\n",
    "        \"I'm sorry, I didn't understand your response.\"\n",
    "        \"\\nPlease provide a full email address or phone number (in the format xxx-xxx-xxxx)\"\n",
    "    ]\n",
    "\n",
    "    # Define a method for generating a greeting message.\n",
    "    def greeting_message(self) -> Optional[MessageOutput]:\n",
    "        # Choose a random static prompt.\n",
    "        prompt = random.choice(self.STATIC_PROMPT)\n",
    "        return MessageOutput(prompt, role=Role.ASSISTANT)\n",
    "\n",
    "    # Define a method for generating a retry message when no edges are found.\n",
    "    def no_edges_found(self, **kwargs) -> Optional[MessageOutput]:\n",
    "        # Choose a random retry prompt.\n",
    "        prompt = random.choice(self.RETRY_PROMPT)\n",
    "        return MessageOutput(prompt, role=Role.ASSISTANT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "181136a2-fefd-4d21-9e08-059aacef399b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BaseNode(abc.ABC, Generic[NodeInput]):\n",
      "\n",
      "    \"\"\"Node\n",
      "    at it's highest level, a node asks a user for some input, and trys\n",
      "    that input on all edges. It also manages and executes all\n",
      "    the edges it contains\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, edges: Optional[List[BaseEdge]] = None, final_state=False):\n",
      "        \"\"\"\n",
      "        prompt (str): what to ask the user\n",
      "        retry_prompt (str): what to ask the user if all edges fail\n",
      "        parse_class (Pydantic BaseModel): the structure of the parse\n",
      "        llm (LangChain LLM): the large language model being used\n",
      "        \"\"\"\n",
      "\n",
      "        self._edges = edges\n",
      "        self._node_input = None\n",
      "        self._final_state = final_state\n",
      "\n",
      "    def is_node_final(self):\n",
      "        return self._final_state\n",
      "\n",
      "    def set_node_input(self, edge_output: EdgeOutput):\n",
      "        self._node_input = edge_output\n",
      "\n",
      "    def run_to_continue(self, user_input: NodeInput) -> Optional[EdgeOutput]:\n",
      "        \"\"\"Run all edges until one continues\n",
      "        returns the result of the continuing edge, or None\n",
      "        \"\"\"\n",
      "        res = None\n",
      "        for edge in self._edges:\n",
      "            res = edge.execute(user_input)\n",
      "            if res is not None and res.should_continue:\n",
      "                return res\n",
      "        return res\n",
      "\n",
      "    def execute(self, user_input: NodeInput) -> Union[MessageOutput, EdgeOutput]:\n",
      "        \"\"\"Handles the current conversational state\n",
      "        prompts the user, tries again, runs edges, etc.\n",
      "        returns the result from an adge\n",
      "        \"\"\"\n",
      "        res = self.run_to_continue(user_input)\n",
      "        if res is None or not res.should_continue:\n",
      "            return self.no_edges_found(user_input)\n",
      "        else:\n",
      "            if res.next_node is not None:\n",
      "                res.next_node.set_node_input(res.result)\n",
      "\n",
      "        return res\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def greeting_message(self) -> Optional[MessageOutput]:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def no_edges_found(self, user_input: NodeInput) -> Optional[MessageOutput]:\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(BaseNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2181210f-4d35-44d1-8318-698434c30f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "greeting_node = GreetingNode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f27c73bf-b623-4951-9083-1d0811890037",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MessageOutput(message='Hi, welcome to our online support, in order to proceed we need to identify you first, could you please input your full email address or phone number', role=<Role.ASSISTANT: 'assistant'>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greeting_node.greeting_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4968c8b9-9c59-48e5-84e6-93cdcd7d85d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a class called 'UserInfoChainBasedEdge' that inherits from 'ZeroShotChainBasedEdge'.\n",
    "class UserInfoChainBasedEdge(ZeroShotChainBasedEdge):\n",
    "    # Define a prefix for the prompt.\n",
    "    _prompt_prefix = \"\"\"Your goal is to find out the user information and their subscription type.\n",
    "- You MUST ALWAYS combine the output of different tools to achieve your final answer.\n",
    "- The user subscription must be either free or premium, never empty\n",
    "\n",
    "To achieve this you have access to the following tools:\"\"\"\n",
    "\n",
    "    # Define a suffix for the prompt.\n",
    "    _prompt_suffix = \"\"\"\\nYour final answer should combine the information of previous Observations \n",
    "{format_instructions}\n",
    "Begin! \n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "    # Define a method to get the available tools.\n",
    "    def _get_tools(self):\n",
    "        tools = [\n",
    "            Tool.from_function(\n",
    "                func=search_user_info_on_db,\n",
    "                description=\"Database tool to search user information, like user id, phone number and etc. Input should be their email as text\",\n",
    "                name=\"user_info_db_search\",\n",
    "            ),\n",
    "            Tool.from_function(\n",
    "                func=search_user_subscription_on_db,\n",
    "                description=\"Database tool to search user subscription type by user id, requires a number as input\",\n",
    "                name=\"user_subscription_db_search\",\n",
    "            ),\n",
    "        ]\n",
    "        return tools\n",
    "\n",
    "    # Define a method to get message output based on the input.\n",
    "    def _get_message_output(\n",
    "        self, msg_input: Union[str, BaseModel]\n",
    "    ) -> List[MessageOutput]:\n",
    "        user_info = msg_input if isinstance(msg_input, str) else str(msg_input)\n",
    "        message = f\"User Info retrieved: {user_info}\"\n",
    "        return [MessageOutput]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a061f1f-00a7-4097-9298-a73119793a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BaseEdge(abc.ABC, Generic[EdgeInput, ResultsType]):\n",
      "    def __init__(self, model, max_retries=3, out_node=None):\n",
      "        self._llm_model = model\n",
      "\n",
      "        # how many times the edge has failed, for any reason, for deciding to skip\n",
      "        # when successful this resets to 0 for posterity.\n",
      "        self._num_fails = 0\n",
      "\n",
      "        # how many retrys are acceptable\n",
      "        self._max_retries = max_retries\n",
      "\n",
      "        # the node the edge directs towards\n",
      "        self._out_node = out_node\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_message_output(\n",
      "        self, msg_input: Union[str, BaseModel]\n",
      "    ) -> Optional[List[MessageOutput]]:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def check(self, model_output: str) -> bool:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _parse(self, model_input: EdgeInput) -> ResultsType:\n",
      "        pass\n",
      "\n",
      "    def _get_edge_output(\n",
      "        self, should_continue: bool, result: Optional[ResultsType]\n",
      "    ) -> EdgeOutput:\n",
      "        message_output = self._get_message_output(result)\n",
      "        return EdgeOutput(\n",
      "            should_continue=should_continue,\n",
      "            result=result,\n",
      "            num_fails=self._num_fails,\n",
      "            next_node=self._out_node,\n",
      "            message_output=message_output,\n",
      "        )\n",
      "\n",
      "    def execute(self, user_input: EdgeInput):\n",
      "        \"\"\"Executes the entire edge\n",
      "        returns a dictionary:\n",
      "        {\n",
      "            continue: bool,       weather or not should continue to next\n",
      "            result: parse_class,  the parsed result, if applicable\n",
      "            num_fails: int        the number of failed attempts\n",
      "            continue_to: Node     the Node the edge continues to\n",
      "        }\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "            # attempting to parse\n",
      "            self._num_fails = 0\n",
      "            return self._get_edge_output(\n",
      "                should_continue=True, result=self._parse(user_input)\n",
      "            )\n",
      "        except OutputParserException as parsing_exception:\n",
      "            # there was some error in parsing.\n",
      "            # note, using the retry or correction parser here might be a good idea\n",
      "            self._num_fails += 1\n",
      "            if self._num_fails >= self._max_retries:\n",
      "                return self._get_edge_output(\n",
      "                    should_continue=True,\n",
      "                    result=MessageOutput(\n",
      "                        parsing_exception.llm_output, role=Role.SYSTEM\n",
      "                    ),\n",
      "                )\n",
      "            return self._get_edge_output(\n",
      "                should_continue=False,\n",
      "                result=MessageOutput(parsing_exception.llm_output, role=Role.SYSTEM),\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(BaseEdge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1c61327-af29-4275-b445-167d6822ac8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ChainBasedEdge(BaseEdge[MessageHistory, MessageOutput], ABC):\n",
      "    def __init__(\n",
      "        self,\n",
      "        model,\n",
      "        pydantic_object: Optional[Type[BaseModel]],\n",
      "        max_retries=3,\n",
      "        out_node=None,\n",
      "    ):\n",
      "        super().__init__(model=model, max_retries=max_retries, out_node=out_node)\n",
      "        if pydantic_object is not None:\n",
      "            self._output_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
      "        else:\n",
      "            self._output_parser = None\n",
      "\n",
      "        self._init_chain()\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _predict(self, model_input: ModelInput) -> str:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _init_chain(self, *kwargs):\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_prompt_template(self) -> BasePromptTemplate:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _prompt_input_variables(self) -> list:\n",
      "        pass\n",
      "\n",
      "    def check(self, model_output: str) -> bool:\n",
      "        return isinstance(self._output_parser.parse(model_output), BaseModel)\n",
      "\n",
      "    def _parse(self, message_history: MessageHistory) -> Union[str, BaseModel]:\n",
      "        model_input = message_history.model_input()\n",
      "        str_to_parse = self._predict(model_input=model_input)\n",
      "        out = (\n",
      "            self._output_parser.parse(str_to_parse)\n",
      "            if self._output_parser is not None\n",
      "            else str_to_parse\n",
      "        )\n",
      "        return out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ChainBasedEdge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f86b832-c419-443b-ae0e-710a624ae2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ZeroShotChainBasedEdge(ChainBasedEdge, ABC):\n",
      "    _prompt_prefix = None\n",
      "    _prompt_suffix = None\n",
      "\n",
      "    def _prompt_input_variables(self):\n",
      "        input_variables = [\"input\", \"agent_scratchpad\", \"history\"]\n",
      "        if self._output_parser is not None:\n",
      "            input_variables += [\"format_instructions\"]\n",
      "\n",
      "        return input_variables\n",
      "\n",
      "    def _get_prompt_template(self) -> BasePromptTemplate:\n",
      "        input_variables = self._prompt_input_variables()\n",
      "        history = \"\"\"Conversation History \\n{history}\\n\"\"\"\n",
      "\n",
      "        prompt = ZeroShotAgent.create_prompt(\n",
      "            tools=self._tools,\n",
      "            prefix=self._prompt_prefix,\n",
      "            suffix=history + self._prompt_suffix,\n",
      "            input_variables=input_variables,\n",
      "        )\n",
      "        return prompt\n",
      "\n",
      "    def _init_chain(self, **kwargs):\n",
      "        self._tools = self._get_tools()\n",
      "\n",
      "        self._prompt = self._get_prompt_template()\n",
      "        self._llm_chain = LLMChain(llm=self._llm_model, prompt=self._prompt)\n",
      "\n",
      "        agent = ZeroShotAgent(llm_chain=self._llm_chain, tools=self._tools)\n",
      "        self._agent_executor = AgentExecutor.from_agent_and_tools(\n",
      "            agent=agent, tools=self._tools, verbose=True, handle_parsing_errors=True\n",
      "        )\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_tools(self):\n",
      "        pass\n",
      "\n",
      "    def _predict(self, model_input: ModelInput) -> str:\n",
      "        if self._output_parser is not None:\n",
      "            result = self._agent_executor.run(\n",
      "                input=model_input.input,\n",
      "                history=model_input.history,\n",
      "                format_instructions=self._output_parser.get_format_instructions(),\n",
      "            )\n",
      "        else:\n",
      "            result = self._agent_executor.run(\n",
      "                input=model_input.input, history=model_input.history\n",
      "            )\n",
      "\n",
      "        return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ZeroShotChainBasedEdge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1edf9ec2-7e6f-498b-ac6d-730403cca81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_info_edge = UserInfoChainBasedEdge(model=llm_model, pydantic_object=UserProfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "941c63b3-78ef-41d0-a1c8-812d88dde2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\"email\": \"michaeljackson@gmail.com\"}\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to use the user_info_db_search tool to search for the user's information based on their email.\n",
      "Action: user_info_db_search\n",
      "Action Input: michaeljackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe user information for michaeljackson@gmail.com is not available in the database. I need to try a different email.\n",
      "Action: user_info_db_search\n",
      "Action Input: mjackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI still couldn't find the user information. Let me try another email.\n",
      "Action: user_info_db_search\n",
      "Action Input: michael.jackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI still couldn't find the user information. Let me try another email.\n",
      "Action: user_info_db_search\n",
      "Action Input: mjackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have exhausted all possible email combinations and still couldn't find the user information. I need to try a different approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: michael.jackson\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI still couldn't find the user information. Let me try another approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: michaeljackson\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have exhausted all possible ways to search for the user information. I need to try a different approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: jackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have exhausted all possible ways to search for the user information. I need to try a different approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: jackson\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have exhausted all possible ways to search for the user information. I need to try a different approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: michaeljackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have exhausted all possible ways to search for the user information. I need to try a different approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: michaeljackson\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have exhausted all possible ways to search for the user information. I need to try a different approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: jackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have exhausted all possible ways to search for the user information. I need to try a different approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: jackson\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have exhausted all possible ways to search for the user information. I need to try a different approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: michael.jackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have exhausted all possible ways to search for the user information. I need to try a different approach.\n",
      "Action: user_info_db_search\n",
      "Action Input: michael.jackson\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EdgeOutput(should_continue=False, result=MessageOutput(message='Agent stopped due to iteration limit or time limit.', role=<Role.SYSTEM: 'system'>), message_output=[<class 'data.graph.MessageOutput'>], num_fails=1, next_node=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info_edge.execute(get_message_history(\"michaeljackson@gmail.com\", Role.USER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5c10555-ed8c-4825-ad09-94976d362df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a class called 'AuthenticatedUserNode' that inherits from 'MultiRetrievalNode'.\n",
    "class AuthenticatedUserNode(MultiRetrievalNode):\n",
    "    # Define a static prompt that includes placeholders for user information.\n",
    "    STATIC_PROMPT = [\n",
    "        \"Hi, {user_name} I am your Shopify Agent for today, you have the {subscription} subscription \"\n",
    "        \"I can help you with any Help or you can ask me to call you at any time!\"\n",
    "    ]\n",
    "\n",
    "    # Initialize the AuthenticatedUserNode.\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model,\n",
    "        pydantic_object: Optional[Type[BaseNode]],\n",
    "        edges: List[BaseEdge] = None,\n",
    "    ):\n",
    "        self._hc_agent = HelpCenterAgent()\n",
    "        super().__init__(llm_model, pydantic_object, edges)\n",
    "\n",
    "    # Define a greeting message for the authenticated user.\n",
    "    def greeting_message(self) -> Optional[MessageOutput]:\n",
    "        # Choose a random static prompt.\n",
    "        prompt = random.choice(self.STATIC_PROMPT)\n",
    "        user_profile: UserProfile = self._node_input\n",
    "\n",
    "        # Replace placeholders in the prompt with user information.\n",
    "        prompt = prompt.format(\n",
    "            user_name=user_profile.name, subscription=user_profile.subscription\n",
    "        )\n",
    "        return MessageOutput(prompt, role=Role.ASSISTANT)\n",
    "\n",
    "    # Define retriever information for different knowledge bases.\n",
    "    def _get_retriever_infos(self):\n",
    "        retriever_infos = [\n",
    "            {\n",
    "                \"name\": \"Premium Subscription Knowledge Base\",\n",
    "                \"description\": \"Contains information for users with a premium subscription\",\n",
    "                \"retriever\": self._hc_agent.paid_sub_retriever(),\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Free Subscription Knowledge Base\",\n",
    "                \"description\": \"Contains information for users with a free subscription\",\n",
    "                \"retriever\": self._hc_agent.free_sub_retriever(),\n",
    "            },\n",
    "        ]\n",
    "        return retriever_infos\n",
    "\n",
    "    # Define a default language model chain.\n",
    "    def _get_default_chain(self):\n",
    "        template = \"\"\"You are a helpful assistant, you should tell the user that his query is outside of your domain \n",
    "    in a friendly way\"\n",
    "    Human: \"\"\"\n",
    "\n",
    "        prompt_template = PromptTemplate.from_template(template)\n",
    "        chain = LLMChain(\n",
    "            llm=self._llm_model, prompt=prompt_template, output_key=\"result\"\n",
    "        )\n",
    "        return chain\n",
    "\n",
    "    # Define a method for handling cases where no edges are found.\n",
    "    def no_edges_found(self, user_input: MessageHistory) -> Optional[MessageOutput]:\n",
    "        # Use a language model to predict a response.\n",
    "        message = self._predict(user_input)\n",
    "        return MessageOutput(message=message, role=Role.ASSISTANT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "acc5d6bb-1317-4c03-a013-ab4d60675d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ChainBasedNode(BaseNode[MessageHistory], abc.ABC):\n",
      "    def __init__(\n",
      "        self,\n",
      "        llm_model,\n",
      "        pydantic_object: Optional[Type[BaseModel]],\n",
      "        edges: Optional[List[BaseEdge]],\n",
      "        final_state=False,\n",
      "    ):\n",
      "        self._llm_model = llm_model\n",
      "        self._parse_class = pydantic_object\n",
      "\n",
      "        if pydantic_object is not None:\n",
      "            self._output_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
      "        else:\n",
      "            self._output_parser = None\n",
      "\n",
      "        self._init_chain()\n",
      "        super().__init__(edges, final_state)\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _init_chain(self, **kwargs):\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ChainBasedNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57b433e2-0927-41f3-8a69-430b71a1fa77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class MultiRetrievalNode(ChainBasedNode, abc.ABC):\n",
      "    @abc.abstractmethod\n",
      "    def _get_retriever_infos(self):\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_default_chain(self):\n",
      "        pass\n",
      "\n",
      "    def _init_chain(self, *kwargs):\n",
      "        retriever_infos = self._get_retriever_infos()\n",
      "\n",
      "        self._llm_chain = MultiRetrievalQAChain.from_retrievers(\n",
      "            self._llm_model,\n",
      "            retriever_infos,\n",
      "            default_chain=self._get_default_chain(),\n",
      "            verbose=True,\n",
      "        )\n",
      "\n",
      "    def _predict(self, messages: MessageHistory) -> str:\n",
      "        return self._llm_chain.run(messages)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(MultiRetrievalNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.6.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (0.16.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: click in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision->sentence-transformers) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vigne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Installing collected packages: transformers, sentence-transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\vigne\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\transformers\\\\models\\\\deprecated\\\\trajectory_transformer\\\\convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\vigne\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2aa7ca62-bb01-465d-91a6-018f044aa617",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain\\embeddings\\huggingface.py:57\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vigne\\Downloads\\customer_support_updated\\customer_support.ipynb Cell 84\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create an instance of the 'AuthenticatedUserNode' class.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m authenticated_user_node \u001b[39m=\u001b[39m AuthenticatedUserNode(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     llm_model\u001b[39m=\u001b[39;49mllm_model,  \u001b[39m# Pass the 'llm_model' as a language model for the node.\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pydantic_object\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Pass 'None' for the 'pydantic_object'.\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     edges\u001b[39m=\u001b[39;49m[],  \u001b[39m# Pass an empty list for 'edges'.\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\vigne\\Downloads\\customer_support_updated\\customer_support.ipynb Cell 84\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     llm_model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     pydantic_object: Optional[Type[BaseNode]],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     edges: List[BaseEdge] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m ):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hc_agent \u001b[39m=\u001b[39m HelpCenterAgent()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y162sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(llm_model, pydantic_object, edges)\n",
      "File \u001b[1;32mc:\\Users\\vigne\\Downloads\\customer_support_updated\\tools\\rag_responder.py:15\u001b[0m, in \u001b[0;36mHelpCenterAgent.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 15\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_free_sub_db \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_index(directory\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mFREE_SUB_DOC_PATH)\n\u001b[0;32m     16\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_paid_sub_db \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_index(directory\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPREMIUM_SUB_DOC_PATH)\n\u001b[0;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qa_chain \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_qa_chain()\n",
      "File \u001b[1;32mc:\\Users\\vigne\\Downloads\\customer_support_updated\\tools\\rag_responder.py:21\u001b[0m, in \u001b[0;36mHelpCenterAgent._create_index\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_index\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     embeddings \u001b[39m=\u001b[39m SentenceTransformerEmbeddings(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mall-MiniLM-L6-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     22\u001b[0m     persist_directory \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchroma_db/\u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_docs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_docs(directory))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain\\embeddings\\huggingface.py:60\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m---> 60\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m     61\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import sentence_transformers python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m sentence_transformers\u001b[39m.\u001b[39mSentenceTransformer(\n\u001b[0;32m     66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name, cache_folder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_folder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_kwargs\n\u001b[0;32m     67\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "# Create an instance of the 'AuthenticatedUserNode' class.\n",
    "\n",
    "authenticated_user_node = AuthenticatedUserNode(\n",
    "    llm_model=llm_model,  # Pass the 'llm_model' as a language model for the node.\n",
    "    pydantic_object=None,  # Pass 'None' for the 'pydantic_object'.\n",
    "    edges=[],  # Pass an empty list for 'edges'.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21347aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'authenticated_user_node' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vigne\\Downloads\\customer_support_updated\\customer_support.ipynb Cell 85\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y163sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m authenticated_user_node\u001b[39m.\u001b[39mexecute(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y163sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     get_message_history(\u001b[39m\"\u001b[39m\u001b[39mUser with Premium Subscription: What is a POS\u001b[39m\u001b[39m\"\u001b[39m, Role\u001b[39m.\u001b[39mUSER)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vigne/Downloads/customer_support_updated/customer_support.ipynb#Y163sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'authenticated_user_node' is not defined"
     ]
    }
   ],
   "source": [
    "authenticated_user_node.execute(\n",
    "    get_message_history(\"User with Premium Subscription: What is a POS\", Role.USER)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
